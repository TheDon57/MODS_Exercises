{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheDon57/MODS_Exercises/blob/main/Week_11_exercise/Topic_Modeling_Week_11_yelp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53a0f57",
      "metadata": {
        "id": "e53a0f57"
      },
      "source": [
        "## Bonuspunktaufgabe 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "2a747bb7-dede-4c21-aaa1-77cce95fd0b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a747bb7-dede-4c21-aaa1-77cce95fd0b6",
        "outputId": "bf493db2-5c78-4ffb-8e41-bf95e57dd860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.tree import plot_tree\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim import corpora\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)  # Set seed for NumPy\n",
        "random.seed(42) # Set seed for random module"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6464f906",
      "metadata": {
        "id": "6464f906"
      },
      "source": [
        "Nutzen Sie dieses Notebook, um die Bonuspunktaufgabe 3 zu bearbeiten. Zuvor werden alle nötigen Packages importiert, die Sie zur Bearbeitung der Aufgabe benötigen.\n",
        "\n",
        "Ziel ist es, die Yelp-Rezensionen mittels Topic Modeling zu analysieren und Topics aus den Daten zu extrahieren. Anschließend sollen diese Informationen genutzt werden, um mit supervised Machine Learning die Bewertung (`stars`) vorherzusagen.\n",
        "\n",
        "Nutzen Sie die untenstehende Codezelle, um diese Aufgabe zu bearbeiten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "b63bb783",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b63bb783",
        "outputId": "ac1e7198-eda1-4627-e00f-1c362bd55adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       id  stars                 date  \\\n",
            "0  ----X0BIDP9tA49U3RvdSQ      4  2014-02-17 00:00:00   \n",
            "1  ---0hl58W-sjVTKi5LghGw      4  2016-07-24 00:00:00   \n",
            "2  ---3OXpexMp0oAg77xWfYA      5  2012-04-07 00:00:00   \n",
            "3  ---65iIIGzHj96QnOh89EQ      5  2015-09-11 00:00:00   \n",
            "4  ---7WhU-FtzSUOje87Y4uw      5  2016-01-22 00:00:00   \n",
            "\n",
            "                                                text             business_id  \\\n",
            "0  Red, white and bleu salad was super yum and a ...  Ue6-WhXvI-_1xUIuapl0zQ   \n",
            "1  Ate the momos during the momo crawl.. Was the ...  Ae4ABFarGMaI5lk1i98A0w   \n",
            "2  Pizza here made my night... Good people and gr...  lKq4Qsz13FDcAVgp49uukQ   \n",
            "3  Great brisket sandwich as claimed. Weird that ...  6nKR80xEGHYf2UxAe_Cu_g   \n",
            "4  Interesting food, great atmosphere, and great ...  Z_mJYg3vi8cPZHa1J4BALw   \n",
            "\n",
            "                  user_id  \n",
            "0  gVmUR8rqUFdbSeZbsg6z_w  \n",
            "1  Y6qylbHq8QJmaCRSlKdIog  \n",
            "2  SnXZkRN9Yf060pNTk1HMDg  \n",
            "3  VcmSgvslHAhqWoEn16wjjw  \n",
            "4  NKF9v-r0jd1p0JVi9h2T1w  \n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "reviews = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS25_26/refs/heads/main/data/yelp.csv\")\n",
        "\n",
        "print(reviews[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X = reviews.drop(columns=['id','user_id','business_id', 'date'])\n",
        "y = reviews['stars']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "lE0PXj_wdTyb"
      },
      "id": "lE0PXj_wdTyb",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def preprocess(text):\n",
        "  # Tokenizing\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "\n",
        "  # Stemming\n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "  # Stopword removal\n",
        "  stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "  filtered_tokens = [token for token in stemmed_tokens if token.lower() not in stopwords]\n",
        "\n",
        "  # Punctuation removal\n",
        "  filtered_tokens_nopunct = [re.sub(r'[^\\w\\s]', '', token) for token in filtered_tokens if token]\n",
        "\n",
        "  return filtered_tokens_nopunct\n",
        "\n",
        "# Apply preprocessing\n",
        "X_train['tokens'] = X_train['text'].apply(preprocess)\n",
        "X_test['tokens'] = X_test['text'].apply(preprocess)"
      ],
      "metadata": {
        "id": "JbjCf6IXePoO"
      },
      "id": "JbjCf6IXePoO",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Dictionary and Corpus\n",
        "dictionary = corpora.Dictionary(X_train['tokens'])\n",
        "dictionary.filter_extremes(no_below=5)\n",
        "\n",
        "corpus_train = [dictionary.doc2bow(text) for text in X_train['tokens']]\n",
        "corpus_test = [dictionary.doc2bow(text) for text in X_test['tokens']]"
      ],
      "metadata": {
        "id": "6ZFXhaFngVEu"
      },
      "id": "6ZFXhaFngVEu",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Topic Model\n",
        "model_10 = LdaModel(corpus=corpus_train, num_topics=10, id2word = dictionary, iterations=10, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJzy1kl1lRIk",
        "outputId": "acd58916-70a6-49a4-da7a-c3d3d867ab54"
      },
      "id": "EJzy1kl1lRIk",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Topic Distributions and Store it in Data Frames\n",
        "def get_document_topic_distribution(model, corpus):\n",
        "  return pd.DataFrame(\n",
        "      [[prob for _, prob in model.get_document_topics(doc, minimum_probability=0)] for doc in corpus],\n",
        "      columns=[f'topic{i+1}' for i in range(model.num_topics)]\n",
        "  )\n",
        "\n",
        "train_topic_distributions = get_document_topic_distribution(model_10, corpus_train)\n",
        "test_topic_distributions = get_document_topic_distribution(model_10, corpus_test)"
      ],
      "metadata": {
        "id": "uNezCtLulzQ8"
      },
      "id": "uNezCtLulzQ8",
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Fit a Decision Tree\n",
        "decision_tree = DecisionTreeRegressor(random_state=42).fit(train_topic_distributions, y_train)"
      ],
      "metadata": {
        "id": "OPBOJ2EBnPzt"
      },
      "id": "OPBOJ2EBnPzt",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate a Decision Tree\n",
        "predictions_tree = decision_tree.predict(test_topic_distributions)\n",
        "\n",
        "rmse_tree = mean_squared_error(y_test, predictions_tree)\n",
        "print(rmse_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG30GGOSo7Xv",
        "outputId": "a9d7fc2c-d0c4-40c5-f1ab-ad9068c77097"
      },
      "id": "KG30GGOSo7Xv",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Fit a Linear Regression\n",
        "train_topic_distributions_ols = train_topic_distributions\n",
        "train_topic_distributions_ols['stars'] = y_train.values\n",
        "test_topic_distributions_ols = test_topic_distributions\n",
        "test_topic_distributions_ols['stars'] = y_test.values\n",
        "\n",
        "linear_model = ols(\n",
        "    formula=\"stars ~ topic1 + topic2 + topic3 + topic4 + topic5 + topic6 + topic7 + topic8 + topic9 + topic10\",\n",
        "    data=train_topic_distributions_ols\n",
        ")\n",
        "linear_model = linear_model.fit()"
      ],
      "metadata": {
        "id": "jUu-5Ckct6JZ"
      },
      "id": "jUu-5Ckct6JZ",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate a Linear Regression\n",
        "predictions_linear = linear_model.predict(test_topic_distributions_ols)\n",
        "\n",
        "rmse_linear = mean_squared_error(y_test, predictions_linear)\n",
        "print(rmse_linear)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euPEflaxwMUI",
        "outputId": "206c7052-5568-4e8c-88c3-2431891daea6"
      },
      "id": "euPEflaxwMUI",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6396564618982181\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "mods",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}